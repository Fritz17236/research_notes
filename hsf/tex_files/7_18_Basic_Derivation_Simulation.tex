\section{Basic Model}
\label{section:derivation:basic_model}
\subsection{Derivation}
\begin{enumerate}
    \item Let $\tau_s$ be the synaptic time constant of each synapse in the network. Define dimensionless time as:
    \begin{equation*}
        \xi \overset{\Delta}{=} \frac{t}{\tau_s}.
    \end{equation*}\\
    We now assume our Linear Dynamical System is expressed in dimensionless time, i.e
    
    \begin{equation}
        \label{eq:lds_dimensionless}
        \frac{dx}{d\xi} = Ax(\xi) + B c(\xi).
    \end{equation}
    
    To describe the neuron dynamics in dimensionless time, let $o(\xi) \in \mathbf{R}^{N}$ be the spike trains of N neurons composing the network with components
    \begin{equation*}
        o_j(\xi) = \sum_{k=1}^{\text{$n_j$ spikes}} \delta(\xi - \xi_{j}^{k}),
    \end{equation*}
    where $\xi_j^k$ is the time at which neuron $j$ makes its $k^{th}$ spike. 
    Define the network's estimate of the state variable as
    \begin{equation}
        \label{eq:xhat}
        \hat{x}(\xi)
        \overset{\Delta}{=} D r(\xi), 
    \end{equation}
    where $D \in \mathbf{R}^{d \times N}$ and 
    \begin{equation}
    \label{eq:rdot}
        \frac{dr}{d \xi} = -r + o(\xi).
    \end{equation}\\
    When the probability of synaptic transmission is $1$, component $r_j$ is the total received post-synaptic current (PSC) from neuron $j$ by the network estimator. 
    Define the network error as
    \begin{equation}
    \label{eq:error_def}
        e(\xi) \overset{\Delta}{=} x(\xi) - \hat{x}(\xi).
    \end{equation}
    
    \item From equations (\ref{eq:rdot}) and (\ref{eq:xhat}), we have
    
    \begin{align*}
        D \dot{r} + D r &= Do \\
        \\
        \implies \dot{\hat{x}} + \hat{x} &= Do,
    \end{align*}
    where the dot denotes derivative w.r.t dimensionless time $\xi$.

    Subtract $\dot{\hat{x}}$ from $\dot{x}$ to get $\dot{e}$:
	\begin{align*}
        \dot{e} &= \dot{x}-\dot{\hat{x}} \\
        &= \left( Ax + Bc \right) - \left( Do - \hat{x} \right) \\
        &= A\left(  e + \hat{x} \right) + Bc - Do + \hat{x} \\
        &= A e + (A + I)\hat{x} + Bc - Do \\
        &=  A e + (A + I) \left(Dr\right) + Bc - Do  \\
        \implies D^T \dot{e} &= D^T A e + D^T (A + I) \left(Dr\right) + D^T Bc - D^T Do. 
	\end{align*} 
	
The quantity $D^T e$ defines the membrane voltage of the predictive coding framework (PCF), a precursor to this model:
$$
	v_{pcf} \overset{\Delta}{=}  D^T e.
$$
Note that the definition implies $e = D^{T \dagger} v_{pcf}$. 
The voltage dynamics are thus

\begin{align}
\label{eq:derivation_init}
\dot{v}_{pcf} 
= 
D^T A D^{T \dagger} v_{pcf} + D^T (A + I) \left(Dr\right) + D^T Bc - D^T Do,
\end{align}

where $D^{T \dagger}$ is the left pseudo-inverse of $D^T \in \mathbf{R}^{N \times d}$. The PCF thus defines a mapping between two vector spaces: the d-dimensional state space of the target system, and the N-dimensional voltage space of the spiking neural network. This mapping is visualized in figure (\ref{fig:derivation:basic_model:pcf_e_v_map}). 
\begin{figure}
    \centering
    \includegraphics[scale=.703]{figures/pcf_e_v_graphic.pdf}
    \caption{Mapping Between State and Voltage Spaces: \textbf{\textit{Top:}} The estimation error $e$ is computed by comparing the decoded network estimate to the true state of the target dynamical system.  \textbf{\textit{Middle:}} The $e$ is projected onto the encoding directions of the neurons composing the network. The projection of error onto encoding direction $j$ gives the membrane voltage of neuron $j$, $v_j = d_j^T e$. \textbf{\textit{Bottom:}} The voltages form a N-dimensional vector contained in voltage space.}
    \label{fig:derivation:basic_model:pcf_e_v_map}
\end{figure}

\clearpage

\item The self-coupled network is derived via a change of bases. Assuming both $D$ and $A$ are full rank, diagonalize each to a common left basis:
\begin{align*}
    A &= \mathcal{U} \Lambda \mathcal{U}^T = \sum_{j=1}^d \Lambda_j \mathcal{U}_j \mathcal{U}_j^T,\\
    \\
    D &= \mathcal{U} \left[S \hspace{2mm} 0 \right]  V^T = \sum_{j=1}^d S_j \mathcal{U}_j  V_j^T,\\
    \\
    D^T &= V \begin{bmatrix} S \\ 0\end{bmatrix} \mathcal{U}^T = \sum_{j=1}^d S_j V_j  \mathcal{U}_j^T, \\
    \\
    D^T D  &= V \begin{bmatrix} S \\ 0\end{bmatrix} \begin{bmatrix} S & 0\end{bmatrix} V^T
     = \sum_{j=1}^d S_j^2 V_j V_j^T , 
\end{align*}
with $\mathcal{U} \in \mathbf{R}^{d \times d}$ and $V \in \mathbf{R}^{N \times N}$, and $S \in \mathbf{R}^{d \times d }$. \\
\\

In the original basis, the state is $x$. In the rotated basis we denote this quantity as $y$. It is the projection of $x$ onto the d-dimensional $\mathcal{U}$ basis:

\begin{align}
\label{eq:definition_rotated_state_space}
y &\overset{\Delta}{=} \mathcal{U}^T x 
%\hat{y} &\overset{\Delta}{=} \mathcal{U}^T \hat{x}.
\end{align}

The rotated target dynamics are thus

\begin{align}
\label{eq:rotated_targed_dynamical_system}
\dot{y} &= \mathcal{U}^T \dot{x} \notag
\\ \notag
\\ \notag
&= 
\Lambda y (\xi)
+
\mathcal{U}^T B c(\xi)
\\ \notag
\\ \notag
&= 
\Lambda y (\xi)
+
\mathcal{U}^T B \mathcal{U} \mathcal{U}^T c(\xi) \notag
\\ \notag
\\ 
&=
\Lambda y (\xi)
+
\beta \tilde{c}(\xi)
\end{align}
where  
$$
\beta \overset{\Delta}{=} \mathcal{U}^T B \mathcal{U},  
$$
and
$$
\tilde{c} \overset{\Delta}{=} \mathcal{U}^T c,
$$
give the projections of $B$ and $c$ respectively.
The network estimate in the rotated basis is 
$$
\hat{y} \overset{\Delta}{=} \mathcal{U}^T \hat{x}.
$$

From equation (\ref{eq:xhat}),
\begin{align*}
\hat{y} &= \mathcal{U}^T \hat{x}
\\ 
\\ 
&=
\mathcal{U}^T D r
\\ 
\\ 
&= 
\begin{bmatrix}
S & 0
\end{bmatrix}
V^T
r
\\
\\
&=
\begin{bmatrix}
S & 0
\end{bmatrix}
\rho
\\
\\
\implies
\dot{\hat{y}}
&= 
\begin{bmatrix}
S & 0
\end{bmatrix}
V^T
\dot{r}
\\
\\
&= 
\begin{bmatrix}
S & 0
\end{bmatrix}
\left( 
-V^T r + V^T o
\right).
\end{align*}

Note that $V^T r$ and $V^T o $ are projections of the N-neuron network's post-synaptic current and spike train respectively onto the rotated basis, denoted by 

\begin{align}
\rho \overset{\Delta}{=} V^T r, 
\label{eq:def_rho}
\\ \notag
\\
\tilde{o} \overset{\Delta}{=} V^T o
\label{eq:def_o_tilde}.
\end{align}

The preceding equality also gives $\hat{y}$ in terms of $\rho$:

\begin{align}
\label{eq:basic:def:y_hat}
\hat{y} = 
\begin{bmatrix}
S & 0
\end{bmatrix}
\rho.
\end{align}

With these definitions, the last equality above also implies

\begin{align}
\label{eq:rho_dot}
\dot{\rho} = -\rho + \tilde{o}.
\end{align}

To finish describing the basic network quantities in terms of the rotated basis, let $\epsilon$ be the error in the rotated basis:
\begin{align}
\label{eq:rotated_error_def}
\epsilon \overset{\Delta}{=} y - \hat{y} \\= \mathcal{U}^T e. \notag
\end{align}

\item Repeat the derivation of equation (\ref{eq:derivation_init}) but with $y$, $\hat{y},$ and $\epsilon$:

\begin{align*}
\dot{\epsilon}
&=
\dot{y} - \dot{\hat{y}}
\\
\\
&= 
\Lambda y + \beta c - 
\begin{bmatrix}
S & 0
\end{bmatrix}
\left(
-\rho + \tilde{o}
\right)
\\
\\
&= 
\Lambda \left(
\epsilon + 
\begin{bmatrix}
S & 0
\end{bmatrix}
\rho
\right)
+ 
\beta c
-
\begin{bmatrix}
S & 0
\end{bmatrix}
\left(
-\rho + \tilde{o}
\right)
\\
\\
&= 
\Lambda \epsilon
+
\left( 
\Lambda + I
\right)
\begin{bmatrix}
S & 0
\end{bmatrix}
\rho
-
\begin{bmatrix}
S & 0
\end{bmatrix}
\tilde{o}
\\
\\
\implies
\begin{bmatrix}
S \\ 0
\end{bmatrix}
\dot{\epsilon}
&= 
\begin{bmatrix}
S \\ 0
\end{bmatrix}
\Lambda \epsilon
+
\begin{bmatrix}
S \\ 0
\end{bmatrix}
\left( 
\Lambda + I
\right)
\begin{bmatrix}
S \\ 0
\end{bmatrix}
\rho
-
\begin{bmatrix}
S \\ 0
\end{bmatrix}
\begin{bmatrix}
S & 0
\end{bmatrix}
\tilde{o}.
\end{align*}

The last equality gives a system of $N$ equations of which only $d$ of are nontrivial. A comparison with equation (\ref{eq:derivation_init}) suggests the N-dimensional rotated membrane potential $v$ is best defined as:

\begin{align}
\label{eq:rotated_voltage_def}
v \overset{\Delta}{=} \begin{bmatrix}
S \\ 0
\end{bmatrix} \epsilon \in \mathbf{R}^N.
\end{align}
\\
This mapping is not invertible unless we only consider the first $d$ components and neglect the remaining, trivial components. Abusing notation, we write  
$$
\epsilon = S^{-1} v, 
$$
giving an $N$ vector whose first $d$ elements are well defined, and the remaining components of $v$ are assumed to be zero. Using a similar abuse for the $\rho$ and $\tilde{o}$ terms, we arrive at the system of $d$ equations describing the nontrivial network voltage dynamics:
\begin{align}
\label{eq:rotated_voltage_dynamics}
\dot{v}
&= 
S\Lambda S^{-1} v + S \left(\Lambda + I \right) S \rho
- S^2 \tilde{o} \notag
\\ \notag
\\
\implies
\dot{v}
&= 
\Lambda v + S \left(\Lambda + I \right) S \rho - S^2 \tilde{o}.
\end{align}

We can also write all $N$ dimensions explicitly to respect the dimensionality of $v$ and $\rho$:

\begin{align*}
\dot{v}
&= 
\begin{bmatrix}
\Lambda & 0
\\
0 & 0
\end{bmatrix}
v +
\begin{bmatrix}
S \left(\Lambda + I_d \right) S & 0
\\
0 & 0
\end{bmatrix}
  \rho - 
 \begin{bmatrix}
S^2 & 0
\\
0 & 0
\end{bmatrix}
    \tilde{o}.
\end{align*}

To summarize conceptually, there are 4 vector spaces in total: the error space which tracks the dynamical system and the network estimate, the voltage space which tracks the membrane potentials, and the transformed counterparts of each in the $\mathcal{U}-V$ bases. Figure (\ref{fig:four_subspace_relation}) shows the relationships derived between these subspaces.

\begin{figure}[h]
\centering
\includegraphics[width=\linewidth]{figures/ev_rotated_classic_graph}
\caption{Depiction of the relationship between original and transformed spaces and their respective error and voltage spaces. An arrow represents left multiplication by the given matrix. The zeros in the full $N \times N$ matrices mapping between $v$ and $\epsilon$ are omitted for clarity.}
\label{fig:four_subspace_relation}
\end{figure}
   
   
\item The spike trains $\tilde{o}$ are chosen minimize the network estimation error
\begin{align}
    \mathcal{L}(\xi) =  || x(\xi + d\xi) - \hat{x}(\xi + d\xi) ||^2. 
\end{align}
The network greedily minimizes $\mathcal{L}(\xi)$ an instant $d\xi$ ahead in time. If no spike occurs at time $\xi$, then the objective is given above. If neuron $j$ spikes, the estimate $\hat{x} \leftarrow \hat{x} + d_j$, where $d_j$ is column $j$ of $D$. The objective is now

\begin{align*}
\mathcal{L}_{sp}(\xi) &= ||x - (\hat{x} + d_j)||^2
\\
\\
&= x^T x - 2 \, x^T \hat{x} - 2 x^T d_j + \hat{x}^T \hat{x} + 2 \hat{x}^T d_j + d_j^T d_j
\\
\\
&= x^T x - 2 x^T \hat{x} + \hat{x}^T \hat{x} - 2 d_j^T(x - \hat{x}) + d_j^T d_j
\\
\\
&= ||x - \hat{x}||^2 - 2 d_j^T(x - \hat{x}) + d_j^T d_j
\\
\\
&= 
\mathcal{L}_{ns}(\xi) - 2 d_j^T(x - \hat{x}) + d_j^T d_j,
\end{align*}
where $\mathcal{L}_{ns}(\xi)$ is the objective if no spike occurs. Spiking occurs when the objective decreases or
\begin{align*}
\mathcal{L}_{sp} &< \mathcal{L}_{ns}
\\
\\
\implies
- 2 d_j^T(x - \hat{x}) &+ d_j^T d_j < 0
\\
\\
\implies
d_j^T(x - \hat{x}) >& \frac{||d_j||^2}{2}.
\end{align*}
Since $d_j^T(x-\hat{x}) = d_j^Te$ is already defined as membrane voltage, the right hand side gives neuron $j's$ spike threshold voltage $v_th$,

$$
v_{th}^{pcf} = \frac{1}{2}
\begin{bmatrix}
d_1^T d_1
\\
\vdots
\\
d_N^T d_N
\end{bmatrix}.
$$

For the rotated network, note $\mathcal{U}^T$ is an orthonormal matrix by definition. Thus it is norm-preserving:

\begin{align*}
\mathcal{L}_{sp}(\xi) &= ||x - \hat{x}||^2
\\
\\
&= 
||\mathcal{U}^T(x - \hat{x})||^2
\\
\\
&= 
||y - \hat{y}||^2.
\end{align*}

If we define the rotated network objective as 
$$
\tilde{L}(\xi) \overset{\Delta}{=} || y(\xi + d\xi) - \hat{y}(\xi + d\xi)||^2,
$$
it is equal to the original network objective when no spike occurs. However, a spike alters the readout by $\hat{y} \leftarrow \hat{y} + S_l$, where $S_l$ is the $l^{th}$ column of $\begin{bmatrix}
S & 0
\end{bmatrix}$. With the same approach as above, the objective when neuron $l$ spikes is

\begin{align*}
\tilde{L}_{sp} &= \tilde{L}_{ns} + 2 S_l^T\epsilon  + S_l^TS_l
\\
\\
\implies
v_l &> \frac{||S_l||^2}{2}.
\end{align*}

This leads to voltage thresholds

$$
v_{th} = \frac{1}{2}
\begin{bmatrix}
S_1^TS_1
\\
\vdots
\\
S_d^T S_d
\\
0
\\
\vdots
\\
0
\end{bmatrix}. 
$$


\item Equations (\ref{eq:rotated_voltage_dynamics}) and (\ref{eq:rho_dot}) describe how we implement a network with d neurons that produces an accurate estimate $\hat{x}$ of the given target system. 


When neuron $l$ spikes, a vector $S_l$ is added to the network estimate, $\hat{y}$. A spike has a strictly positive area so that the network is only able to modify its estimate by adding from a fixed set of vectors.  This restricts the space representable by the network to strictly positive state-space, or only $\frac{1}{2^d}$ dimensions of the desired state-space. To remove this restriction, we add an additional d neurons whose preferred directions $-S_l$ are anti-parallel to neurons $l$ for $l=1, \ldots, d$. Thus the number of neurons required to represent a d-dimensional system is $2d$. We update $U$, $S$, $\Lambda$ and $v_{th}$ to reflect the additional neurons:
\begin{align*}
    U &\leftarrow \left[ U \hspace{2mm} -U\right] \in \mathbf{R}^{d \times 2 d},\\
    \\
    S &\leftarrow
    \begin{bmatrix}
    S & 0 \\ 0 & S
    \end{bmatrix}
    \in \mathbf{R}^{2 d \times 2 d},\\
    \\
    \Lambda &\leftarrow
    \begin{bmatrix}
    \Lambda & 0 \\ 0 & \Lambda
    \end{bmatrix}
    \in \mathbf{R}^{2 d \times 2 d},\\
    \\
    v_{th} &\leftarrow 
    \begin{bmatrix}
    v_{th} \\ v_{th}
    \end{bmatrix} \in \mathbf{R}^{2d},
\end{align*}
and afterward recompute $\beta \in \mathbf{R}^{2 d \times d}$. 

\end{enumerate}

\subsection{Simulation of Basic Equations}
Here we simulate the above equations (\ref{eq:rotated_voltage_dynamics}) and (\ref{eq:rho_dot}) with the $N = 2d$ neurons. The parameters are
\begin{align}
\label{eq:sim_I_params}
A
&=
\ -\begin{bmatrix}  
1 & 0 \\
0 & 1
\end{bmatrix} = \mathcal{U} \Lambda \mathcal{U}^T \notag,
\\
\notag
\\
B
&=
\begin{bmatrix}  
1 & 0 \\
0 & 1
\end{bmatrix}, \notag 
\\
\notag 
\\
c(\xi) 
&=
\begin{bmatrix} 
cos(\frac{\pi}{4} \xi)\\
sin(\frac{\pi}{4} \xi)
\end{bmatrix} 
\\
\notag
\\
D
&=
\mathcal{U} 
\begin{bmatrix}
S & 0
\end{bmatrix}
V^T
=
\mathcal{U} 
\begin{bmatrix}
.1 \, I_d & 0
\end{bmatrix}
I_N \notag,
\\
\notag 
\\
d\xi 
&= 
10^{-6}, \notag 
\\
\notag 
\\
N 
&= 
4,\notag 
\\
\notag 
\\
x(0) 
&= 
\begin{bmatrix} \frac{1}{2} & \frac{1}{2} \end{bmatrix}.\notag 
\end{align}

\begin{figure}
    \centering
    \includegraphics[width=.75\linewidth]{figures/network_decode.png}

    \includegraphics[width=.75\linewidth]{figures/decode_error.png}

    \includegraphics[width=.7\linewidth]{figures/membrane_potential_image.png}
\end{figure}

\newpage

\captionof{figure}{Simulation of equations (\ref{eq:rotated_voltage_dynamics}) and
    (\ref{eq:rho_dot}) with parameters listed in equation (\ref{eq:sim_I_params}). \textbf{\textit{Top:}} The decoded network estimate plotted alongside the target dynamical system. \textbf{\textit{Middle:}} The estimation error along each state-space dimension. \textbf{\textit{Bottom: }}The membrane potentials of the 4 neurons during the same time period.\\
    For the numerical implementation, the matrix exponential was used to integrate the continuous terms over a simulation time step. Continuous terms include all equation terms excepting the delta functions $\omega$ handled separately. After integrating over a timestep, any neuron above threshold was manually reset (action of fast inhibition). If multiple neurons are above threshold, the system is integrated backwards in time until only one neuron is above threshold before spiking. The matrix exponential was computed using a Pad\'{e} approximation via the Python package Scipy: \textit{scipy.linalg.expm()}. 
    } 
    \label{fig:Simulation_I}