\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {2}Derivation of the Basic Model}{3}{section.2}\protected@file@percent }
\newlabel{section:derivation:basic_model}{{2}{3}{Derivation of the Basic Model}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Predecessor: PCF Voltage Dynamics}{3}{subsection.2.1}\protected@file@percent }
\newlabel{eq:lds_dimensionless}{{2.1}{3}{Predecessor: PCF Voltage Dynamics}{equation.2.1}{}}
\newlabel{eq:xhat}{{2.2}{3}{Predecessor: PCF Voltage Dynamics}{equation.2.2}{}}
\newlabel{eq:rdot}{{2.3}{3}{Predecessor: PCF Voltage Dynamics}{equation.2.3}{}}
\newlabel{eq:error_def}{{2.4}{3}{Predecessor: PCF Voltage Dynamics}{equation.2.4}{}}
\newlabel{eq:derivation_init}{{2.5}{4}{Predecessor: PCF Voltage Dynamics}{equation.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Mapping Between State and Voltage Spaces: \textbf  {\textit  {Top:}} The estimation error $e$ is computed by comparing the decoded network estimate to the true state of the target dynamical system. \textbf  {\textit  {Middle:}} The $e$ is projected onto the encoding directions of the neurons composing the network. The projection of error onto encoding direction $j$ gives the membrane voltage of neuron $j$, $v_j = d_j^T e$. \textbf  {\textit  {Bottom:}} The voltages form a N-dimensional vector contained in voltage space.\relax }}{5}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:derivation:basic_model:pcf_e_v_map}{{1}{5}{Mapping Between State and Voltage Spaces: \textbf {\textit {Top:}} The estimation error $e$ is computed by comparing the decoded network estimate to the true state of the target dynamical system. \textbf {\textit {Middle:}} The $e$ is projected onto the encoding directions of the neurons composing the network. The projection of error onto encoding direction $j$ gives the membrane voltage of neuron $j$, $v_j = d_j^T e$. \textbf {\textit {Bottom:}} The voltages form a N-dimensional vector contained in voltage space.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}The Self-Coupled Model is PCF in an Orthonormal Basis}{6}{subsection.2.2}\protected@file@percent }
\newlabel{eq:definition_rotated_state_space}{{2.6}{6}{The Self-Coupled Model is PCF in an Orthonormal Basis}{equation.2.6}{}}
\newlabel{eq:rotated_targed_dynamical_system}{{2.7}{6}{The Self-Coupled Model is PCF in an Orthonormal Basis}{equation.2.7}{}}
\newlabel{eq:def_rho}{{2.8}{7}{The Self-Coupled Model is PCF in an Orthonormal Basis}{equation.2.8}{}}
\newlabel{eq:def_o_tilde}{{2.9}{7}{The Self-Coupled Model is PCF in an Orthonormal Basis}{equation.2.9}{}}
\newlabel{eq:basic:def:y_hat}{{2.10}{7}{The Self-Coupled Model is PCF in an Orthonormal Basis}{equation.2.10}{}}
\newlabel{eq:rho_dot}{{2.11}{7}{The Self-Coupled Model is PCF in an Orthonormal Basis}{equation.2.11}{}}
\newlabel{eq:rotated_error_def}{{2.12}{7}{The Self-Coupled Model is PCF in an Orthonormal Basis}{equation.2.12}{}}
\newlabel{eq:rotated_voltage_def}{{2.13}{8}{The Self-Coupled Model is PCF in an Orthonormal Basis}{equation.2.13}{}}
\newlabel{eq:rotated_voltage_dynamics}{{2.14}{8}{The Self-Coupled Model is PCF in an Orthonormal Basis}{equation.2.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Depiction of the relationship between original and transformed spaces and their respective error and voltage spaces. An arrow represents left multiplication by the given matrix. The zeros in the full $N \times N$ matrices mapping between $v$ and $\epsilon $ are omitted for clarity.\relax }}{9}{figure.caption.3}\protected@file@percent }
\newlabel{fig:four_subspace_relation}{{2}{9}{Depiction of the relationship between original and transformed spaces and their respective error and voltage spaces. An arrow represents left multiplication by the given matrix. The zeros in the full $N \times N$ matrices mapping between $v$ and $\epsilon $ are omitted for clarity.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Optimizing Spike-Timing: From PCF to Self-Coupled}{10}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Consequences of Positive Unit-Area Spikes}{11}{subsection.2.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Sketch of the Encoding space $\epsilon _{sp}$ of neuron $l$ with direction $S_l \in \mathbf  {R}^2.$ The radius of the circle is $v_{th}=\frac  {||S_l||^2}{2}$. (Vector $S_l$ not drawn to scale).\relax }}{12}{figure.caption.4}\protected@file@percent }
\newlabel{fig:relu_encoding_demo}{{3}{12}{Sketch of the Encoding space $\epsilon _{sp}$ of neuron $l$ with direction $S_l \in \mathbf {R}^2.$ The radius of the circle is $v_{th}=\frac {||S_l||^2}{2}$. (Vector $S_l$ not drawn to scale).\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Visualizing $D$ as a sequence of linear maps between subspaces. \textbf  {\textit  {Top: }} The matrix $D \in \mathbf  {R}^{d \times N}$ is decomposed via SVD into a sequence of 3 linear maps (matrices). The rightmost matrix $V^T \in \mathbf  {R}^{N \times N}$ projects a vector $x$ to give coefficients for the expansion in the basis $V$. The center matrix $\begin  {bmatrix} S & 0 \end  {bmatrix} \in \mathbf  {R}^{d \times N}$ maps vectors from the $V$ basis to a vector in $\mathbf  {R}^d$ by scaling and truncation. The leftmost matrix $\mathcal  {U} \in \mathbf  {R}^{d \times d}$ gives the resultant vector $D x \in \mathbf  {R}^d$ by using the scaled vector $\begin  {bmatrix} S & 0 \end  {bmatrix} V^T$ as coefficients for a basis expansion in $\mathcal  {U}$. \textbf  {\textit  {Bottom:}} We rotate the basis for vectors in $\mathbf  {R}^N$ and $\mathbf  {R}^d$ to the $\mathcal  {U}$ and $V$ bases respectively. This negates the need of $D$ to preemptively project and afterward rotate a vector, leaving only scaling by a diagonal matrix. The mapping $D$ performs on a vector $y$ simplifies to multiplication by a diagonal matrix $S$ of $y$'s first $d$ components. \relax }}{15}{figure.caption.5}\protected@file@percent }
\newlabel{fig:linear_maps_between_subspaces_D_real}{{4}{15}{Visualizing $D$ as a sequence of linear maps between subspaces. \textbf {\textit {Top: }} The matrix $D \in \mathbf {R}^{d \times N}$ is decomposed via SVD into a sequence of 3 linear maps (matrices). The rightmost matrix $V^T \in \mathbf {R}^{N \times N}$ projects a vector $x$ to give coefficients for the expansion in the basis $V$. The center matrix $\begin {bmatrix} S & 0 \end {bmatrix} \in \mathbf {R}^{d \times N}$ maps vectors from the $V$ basis to a vector in $\mathbf {R}^d$ by scaling and truncation. The leftmost matrix $\mathcal {U} \in \mathbf {R}^{d \times d}$ gives the resultant vector $D x \in \mathbf {R}^d$ by using the scaled vector $\begin {bmatrix} S & 0 \end {bmatrix} V^T$ as coefficients for a basis expansion in $\mathcal {U}$. \textbf {\textit {Bottom:}} We rotate the basis for vectors in $\mathbf {R}^N$ and $\mathbf {R}^d$ to the $\mathcal {U}$ and $V$ bases respectively. This negates the need of $D$ to preemptively project and afterward rotate a vector, leaving only scaling by a diagonal matrix. The mapping $D$ performs on a vector $y$ simplifies to multiplication by a diagonal matrix $S$ of $y$'s first $d$ components. \relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Balance Between Excitatory and Inhibitory Activity for Oscilllatory Input. \textbf  {\textit  {(a):}} A sinusoidal input $y$ is divided into its positive (excitatory) and negative (inhibitory) components.\textbf  {\textit  {(b):}} Each neuron encodes its respective input by spiking to produce a nonnegative filtered spike train $\rho $. \textbf  {\textit  {(c):}} \relax }}{20}{figure.caption.6}\protected@file@percent }
\newlabel{fig:ei_balance_demo}{{5}{20}{Balance Between Excitatory and Inhibitory Activity for Oscilllatory Input. \textbf {\textit {(a):}} A sinusoidal input $y$ is divided into its positive (excitatory) and negative (inhibitory) components.\textbf {\textit {(b):}} Each neuron encodes its respective input by spiking to produce a nonnegative filtered spike train $\rho $. \textbf {\textit {(c):}} \relax }{figure.caption.6}{}}
\newlabel{eq:expanded_voltage_dynamics}{{2.16}{22}{Consequences of Positive Unit-Area Spikes}{equation.2.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Simulation of Basic Equations}{23}{subsection.2.5}\protected@file@percent }
\newlabel{eq:sim_I_params}{{2.17}{23}{Simulation of Basic Equations}{equation.2.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Simulation of equations (\ref  {eq:rotated_voltage_dynamics}) and (\ref  {eq:rho_dot}) with parameters listed in equation (\ref  {eq:sim_I_params}). \textbf  {\textit  {Top:}} The decoded network estimate plotted alongside the target dynamical system. \textbf  {\textit  {Middle:}} The estimation error along each state-space dimension. \textbf  {\textit  {Bottom: }}The membrane potentials of the 4 neurons during the same time period.  For the numerical implementation, the matrix exponential was used to integrate the continuous terms over a simulation time step. Continuous terms include all equation terms excepting the delta functions $\omega $ handled separately. After integrating over a timestep, any neuron above threshold was manually reset (action of fast inhibition). If multiple neurons are above threshold, the system is integrated backwards in time until only one neuron is above threshold before spiking. The matrix exponential was computed using a Pad\'{e} approximation via the Python package Scipy: \textit  {scipy.linalg.expm()}. \relax }}{25}{figure.6}\protected@file@percent }
\newlabel{fig:Simulation_I}{{6}{25}{Simulation of equations (\ref {eq:rotated_voltage_dynamics}) and (\ref {eq:rho_dot}) with parameters listed in equation (\ref {eq:sim_I_params}). \textbf {\textit {Top:}} The decoded network estimate plotted alongside the target dynamical system. \textbf {\textit {Middle:}} The estimation error along each state-space dimension. \textbf {\textit {Bottom: }}The membrane potentials of the 4 neurons during the same time period.\\ For the numerical implementation, the matrix exponential was used to integrate the continuous terms over a simulation time step. Continuous terms include all equation terms excepting the delta functions $\omega $ handled separately. After integrating over a timestep, any neuron above threshold was manually reset (action of fast inhibition). If multiple neurons are above threshold, the system is integrated backwards in time until only one neuron is above threshold before spiking. The matrix exponential was computed using a Pad\'{e} approximation via the Python package Scipy: \textit {scipy.linalg.expm()}. \relax }{figure.6}{}}
\@setckpt{tex_files/7_18_Basic_Derivation_Simulation}{
\setcounter{page}{26}
\setcounter{equation}{17}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{section}{2}
\setcounter{subsection}{5}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{6}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{caption@flags}{0}
\setcounter{continuedfloat}{0}
\setcounter{Item}{17}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{7}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{section@level}{2}
}
